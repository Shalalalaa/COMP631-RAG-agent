{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijswNMgTfLQt",
        "outputId": "6c824e4b-c932-4eb6-c4e0-9f94d44ecd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioGbxqoHe3FA",
        "outputId": "7dcc9613-9088-4657-ca13-284c90580e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "{\n",
            "  \"q1\": {\n",
            "    \"65\": 0.8136338591575623,\n",
            "    \"9318\": 0.7953237295150757,\n",
            "    \"82\": 0.7892376780509949,\n",
            "    \"8366\": 0.7865391969680786,\n",
            "    \"8869\": 0.7863246202468872\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ æœ€ç»ˆç‰ˆ Memmap Retriever (Colab/Drive å®‰å…¨ç‰ˆ)\n",
        "# å®Œå…¨å¤„ç†å¥½è®¾å¤‡é—®é¢˜ + æ”¯æŒæ–­ç‚¹ç»­è·‘ + æ”¯æŒç›´æ¥æ£€ç´¢ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆï¼\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from huggingface_hub import hf_hub_download\n",
        "from google.colab import drive\n",
        "\n",
        "# ------------------------------\n",
        "# æŒ‚è½½ Google Drive\n",
        "# ------------------------------\n",
        "drive.mount('/content/drive')\n",
        "MEMMAP_DIR = \"/content/drive/MyDrive/your_project_folder/embeddings\"  # ğŸ”¥ ä¿®æ”¹æˆä½ æƒ³æ”¾çš„ä½ç½®\n",
        "\n",
        "# ------------------------------\n",
        "# åŠ è½½è¯­æ–™\n",
        "# ------------------------------\n",
        "def load_corpus(repo_id: str, filename: str, repo_type: str = \"dataset\"):\n",
        "    corpus_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=repo_type)\n",
        "    corpus = {}\n",
        "    with open(corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            doc = json.loads(line)\n",
        "            doc_id = doc.get(\"_id\")\n",
        "            title = doc.get(\"title\", \"\")\n",
        "            text = doc.get(\"text\", \"\")\n",
        "            corpus[doc_id] = {\"title\": title, \"text\": text}\n",
        "    doc_ids = list(corpus.keys())\n",
        "    return corpus, doc_ids\n",
        "\n",
        "# ------------------------------\n",
        "# æ„å»º memmapï¼ˆæ”¯æŒæ–­ç‚¹ç»­è·‘ï¼‰\n",
        "# ------------------------------\n",
        "def build_embeddings_memmap(corpus, doc_ids, model_name: str, batch_size: int = 32, memmap_dir: str = \"embeddings\"):\n",
        "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
        "    d = model.get_sentence_embedding_dimension()\n",
        "    N = len(doc_ids)\n",
        "\n",
        "    os.makedirs(memmap_dir, exist_ok=True)\n",
        "    memmap_path = os.path.join(memmap_dir, \"corpus_emb.dat\")\n",
        "    doc_ids_path = os.path.join(memmap_dir, \"corpus_doc_ids.json\")\n",
        "    progress_path = os.path.join(memmap_dir, \"progress.json\")\n",
        "\n",
        "    if not os.path.exists(memmap_path):\n",
        "        np.memmap(memmap_path, dtype='float32', mode='w+', shape=(N, d))\n",
        "\n",
        "    mmap = np.memmap(memmap_path, dtype='float32', mode='r+', shape=(N, d))\n",
        "\n",
        "    start_idx = 0\n",
        "    if os.path.exists(progress_path):\n",
        "        with open(progress_path, 'r', encoding='utf-8') as pf:\n",
        "            progress = json.load(pf)\n",
        "            start_idx = progress.get('last_idx', 0)\n",
        "        print(f\"ğŸ”„ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œä» {start_idx} ç»§ç»­\")\n",
        "\n",
        "    if not os.path.exists(doc_ids_path):\n",
        "        with open(doc_ids_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(doc_ids, f, ensure_ascii=False)\n",
        "\n",
        "    total_batches = (N + batch_size - 1) // batch_size\n",
        "    for i in tqdm(range(start_idx, N, batch_size), initial=start_idx//batch_size, total=total_batches):\n",
        "        batch_ids = doc_ids[i:i+batch_size]\n",
        "        batch_texts = [corpus[_id][\"text\"] for _id in batch_ids]\n",
        "        embeddings = model.encode(\n",
        "            batch_texts,\n",
        "            convert_to_numpy=True,\n",
        "            batch_size=batch_size,\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "        mmap[i:i+len(embeddings)] = embeddings\n",
        "        mmap.flush()\n",
        "        with open(progress_path, 'w', encoding='utf-8') as pf:\n",
        "            json.dump({'last_idx': i + len(embeddings)}, pf)\n",
        "\n",
        "    if os.path.exists(progress_path):\n",
        "        os.remove(progress_path)\n",
        "    print(\"âœ… å…¨é‡å†™å…¥å®Œæˆ\")\n",
        "\n",
        "    return memmap_path, doc_ids_path, d, N\n",
        "\n",
        "# ------------------------------\n",
        "# Memmap Retriever\n",
        "# ------------------------------\n",
        "class MemmapRetriever:\n",
        "    def __init__(self, memmap_path, doc_ids_path, dimension, num_docs, model_name):\n",
        "        self.dimension = dimension\n",
        "        self.num_docs = num_docs\n",
        "        self.mmap = np.memmap(memmap_path, dtype='float32', mode='r', shape=(num_docs, dimension))\n",
        "        self.corpus_embeddings = torch.from_numpy(self.mmap)\n",
        "        with open(doc_ids_path, 'r', encoding='utf-8') as f:\n",
        "            self.doc_ids = json.load(f)\n",
        "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
        "\n",
        "    def search(self, queries: dict, top_k: int = 5, score_function: str = 'cos_sim'):\n",
        "        query_ids = list(queries.keys())\n",
        "        query_texts = [queries[q] for q in query_ids]\n",
        "        query_emb = self.model.encode(\n",
        "            query_texts,\n",
        "            convert_to_tensor=True,\n",
        "            batch_size=32,\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        ).to('cpu')  # ä¿è¯ query_emb åœ¨ CPU\n",
        "\n",
        "        if score_function == 'cos_sim':\n",
        "            sim = util.cos_sim(query_emb, self.corpus_embeddings)\n",
        "        elif score_function == 'dot':\n",
        "            sim = torch.matmul(query_emb, self.corpus_embeddings.T)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported score_function {score_function}\")\n",
        "\n",
        "        results = {}\n",
        "        for idx, qid in enumerate(query_ids):\n",
        "            topk = torch.topk(sim[idx], k=top_k)\n",
        "            ids, scores = topk.indices.tolist(), topk.values.tolist()\n",
        "            results[qid] = {self.doc_ids[i]: s for i, s in zip(ids, scores)}\n",
        "        return results\n",
        "\n",
        "# ------------------------------\n",
        "# ä¸»ç¨‹åº\n",
        "# ------------------------------\n",
        "if __name__ == '__main__':\n",
        "    repo_id = \"COMP631GroupSYCZ/Corpus\"\n",
        "    filename = \"corpus.jsonl\"\n",
        "    model_name = \"Lajavaness/bilingual-embedding-small\"\n",
        "\n",
        "    if not (os.path.exists(os.path.join(MEMMAP_DIR, \"corpus_emb.dat\")) and os.path.exists(os.path.join(MEMMAP_DIR, \"corpus_doc_ids.json\"))):\n",
        "        corpus, doc_ids = load_corpus(repo_id=repo_id, filename=filename)\n",
        "        build_embeddings_memmap(corpus, doc_ids, model_name=model_name, batch_size=32, memmap_dir=MEMMAP_DIR)\n",
        "\n",
        "    retriever = MemmapRetriever(\n",
        "        memmap_path=os.path.join(MEMMAP_DIR, \"corpus_emb.dat\"),\n",
        "        doc_ids_path=os.path.join(MEMMAP_DIR, \"corpus_doc_ids.json\"),\n",
        "        dimension=384,\n",
        "        num_docs=len(json.load(open(os.path.join(MEMMAP_DIR, \"corpus_doc_ids.json\")))),\n",
        "        model_name=model_name\n",
        "    )\n",
        "\n",
        "    queries = {\n",
        "        \"q1\": \"æˆ‘æ˜¨æ™šæ¢¦è§é£ç¿”çš„é±¼å’Œå¥‡æ€ªçš„å»ºç­‘ï¼Œæƒ³äº†è§£è¿™ä¸¤ä¸ªæ¢¦å¢ƒçš„æ„ä¹‰ã€‚\"\n",
        "    }\n",
        "    results = retriever.search(queries, top_k=5)\n",
        "    print(json.dumps(results, ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQiD_YluSTVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ£€ç´¢å®Œæˆåï¼Œæ‹¿åˆ° results\n",
        "results = retriever.search(queries, top_k=3)\n",
        "\n",
        "# åŠ è½½åŸ corpusï¼ˆåŸæ–‡æ¡£å†…å®¹ï¼‰\n",
        "# corpus, _, _ = load_corpus(\n",
        "#     repo_id=\"COMP631GroupSYCZ/Corpus\",\n",
        "#     filename=\"corpus.jsonl\"\n",
        "# )\n",
        "corpus, _ = load_corpus(\n",
        "    repo_id=\"COMP631GroupSYCZ/Corpus\",\n",
        "    filename=\"corpus.jsonl\"\n",
        ")\n",
        "\n",
        "# æŠŠç»“æœæ ¹æ® doc_idè¿˜åŸæˆ æ–‡æœ¬å†…å®¹\n",
        "for query_id, doc_scores in results.items():\n",
        "    print(f\"ğŸ” æŸ¥è¯¢: {query_id}\")\n",
        "    for doc_id, score in doc_scores.items():\n",
        "        text = corpus[str(doc_id)]['text'][:200]  # åªæ‰“å°å‰200å­—ç¬¦\n",
        "        print(f\"ğŸ“„ æ–‡æ¡£ID: {doc_id}, ç›¸ä¼¼åº¦: {score:.4f}\")\n",
        "        print(f\"å†…å®¹: {text}\")\n",
        "        print(\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMgigo7tSW6p",
        "outputId": "bb6f5804-24eb-4a5e-b94d-be6f26fbde33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” æŸ¥è¯¢: q1\n",
            "ğŸ“„ æ–‡æ¡£ID: 65, ç›¸ä¼¼åº¦: 0.7896\n",
            "å†…å®¹: æ¢¦è§é£é±¼æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿåšæ¢¦æ¢¦è§é£é±¼å¥½ä¸å¥½ï¼Ÿæ¢¦è§é£é±¼æœ‰ç°å®çš„å½±å“å’Œååº”ï¼Œä¹Ÿæœ‰æ¢¦è€…çš„ä¸»è§‚æƒ³è±¡ï¼Œè¯·çœ‹ä¸‹é¢ç”±å‘¨å…¬è§£æ¢¦å®˜ç½‘æ•´ç†çš„æ¢¦è§é£é±¼çš„è¯¦ç»†è§£è¯´å§ã€‚\n",
            "é£é±¼ä¸æ˜¯çœŸçš„èƒ½é£è€Œæ˜¯ä¸€ç§è·³è·ƒæ»‘ç¿”çš„è¿‡ç¨‹ï¼Œè®©ä»€ä¹ˆçœ‹èµ·æ¥åƒæ˜¯åœ¨é£ã€‚åœ¨æ¢¦ä¸­ï¼Œé£é±¼å¾€å¾€æ˜¯ä¸€ç§è¶…è¶Šçš„ç²¾ç¥ä½“ç°ã€‚\n",
            "æ¢¦è§å¤§æµ·ä¸Šå¾ˆå¤šé£é±¼æˆç¾¤ç»“é˜Ÿåœ°åœ¨è·³è·ƒï¼Œé¢„ç¤ºç€è‡ªå·±æœ€è¿‘åšäº‹ä¼šè¶…è¶Šè‡ªå·±çš„é¢„æ–™ã€‚å¾—åˆ°å¾ˆå¥½çš„è¯„ä»·ã€‚\n",
            "æ¢¦è§é£é±¼å‡ºç°åœ¨æ²™æ¼ ä¸­ï¼Œè¡¨ç¤ºè‡ªå·±æœ€è¿‘ä¼šè§£å†³æ‰ä¸€äº›çƒ¦æ¼è‡ªå·±å¾ˆä¹…çš„äº‹æƒ…ã€‚\n",
            "\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ğŸ“„ æ–‡æ¡£ID: 9318, ç›¸ä¼¼åº¦: 0.7724\n",
            "å†…å®¹: æ¢¦è§é±¼åœ¨ç©ºä¸­é£æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿåšæ¢¦æ¢¦è§é±¼åœ¨ç©ºä¸­é£å¥½ä¸å¥½ï¼Ÿæ¢¦è§é±¼åœ¨ç©ºä¸­é£æœ‰ç°å®çš„å½±å“å’Œååº”ï¼Œä¹Ÿæœ‰æ¢¦è€…çš„ä¸»è§‚æƒ³è±¡ï¼Œè¯·çœ‹ä¸‹é¢ç”±å‘¨å…¬è§£æ¢¦å®˜ç½‘æ•´ç†çš„æ¢¦è§é±¼åœ¨ç©ºä¸­é£çš„è¯¦ç»†è§£è¯´å§ã€‚\n",
            "æ¢¦è§é±¼åœ¨å¤©ä¸Šé£ï¼Œè¡¨ç¤ºè¿‘æœŸæœ‰å¾ˆå¤šè‡´å¯Œçš„æœºä¼šæœ‰ä½ é¢å‰ï¼Œè¦å¥½å¥½æŠŠæ¡ã€‚\n",
            "ä¼ è¯´ï¼Œé²¤é±¼åªè¦èƒ½è·ƒè¿‡é¾™é—¨ï¼Œå°±ä¼šåŒ–ä¸ºå¤©ä¸Šçš„é£é¾™;åä»¥â€œé²¤é±¼è·³é¾™é—¨â€æ¯”å–»ä¸­ä¸¾ã€å‡å®˜ç­‰é£é»„è…¾è¾¾ä¹‹äº‹;å¦‚ä»Šåˆç”¨ä½œæ¯”å–»é€†æµå‰è¿›ï¼Œå¥‹å‘å‘ä¸Šï¼Œæ­¥æ­¥é«˜å‡ï¼Œå®˜è¿äº¨é€šã€‚\n",
            "æ¢¦è§é²¤é±¼è·³é¾™é—¨ï¼Œé¢„ç¤ºæ¢¦\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
            "ğŸ“„ æ–‡æ¡£ID: 54, ç›¸ä¼¼åº¦: 0.7679\n",
            "å†…å®¹: æ¢¦è§æ€ªé±¼æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿåšæ¢¦æ¢¦è§æ€ªé±¼å¥½ä¸å¥½ï¼Ÿæ¢¦è§æ€ªé±¼æœ‰ç°å®çš„å½±å“å’Œååº”ï¼Œä¹Ÿæœ‰æ¢¦è€…çš„ä¸»è§‚æƒ³è±¡ï¼Œè¯·çœ‹ä¸‹é¢ç”±å‘¨å…¬è§£æ¢¦å®˜ç½‘æ•´ç†çš„æ¢¦è§æ€ªé±¼çš„è¯¦ç»†è§£è¯´å§ã€‚\n",
            "æ¢¦è§æ€ªé±¼ï¼Œé±¼ä»£è¡¨å†…å¿ƒçªç„¶å‡ºç°çš„æƒ³æ³•ï¼Œæˆ–è€…ç”Ÿæ´»ä¸­æ–°å‡ºç°çš„äººï¼Œé•¿ç›¸å‡¶æ¶ï¼Œæ„Ÿè§‰æå…·æ”»å‡»æ€§ï¼Œè¯´æ˜ä½ æœ€è¿‘çš„æƒ…ç»ªå¹¶ä¸ç¨³å®šï¼Œå—åˆ°ä¸€äº›æƒ³æ³•æˆ–äº‹ç‰©çš„å½±å“ã€‚\n",
            "æ¢¦è§å¤§æ€ªé±¼ï¼Œé¢„ç¤ºç€ä½ æœ€è¿‘åˆéº»çƒ¦äº‹ï¼Œæç¤ºä½ ï¼Œæœ€è¿‘åšäº‹å°å¿ƒä¸ºæ˜¯ã€‚\n",
            "æ¢¦è§æŠ“åˆ°æ€ªé±¼ï¼Œèº«ä½“æœ‰æ¯›ç—…ï¼Œè‚¾è™šï¼Œæƒ³çœçš„é’±æ²¡çœæˆã€‚http:/\n",
            "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å…ˆæŒ‚è½½ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹\n",
        "!mkdir -p /content/drive/MyDrive/your_project_folder/embeddings\n",
        "\n",
        "# å¤åˆ¶æ–‡ä»¶åˆ° Drive\n",
        "!cp /content/embeddings/corpus_emb.dat /content/drive/MyDrive/your_project_folder/embeddings/\n",
        "!cp /content/embeddings/corpus_doc_ids.json /content/drive/MyDrive/your_project_folder/embeddings/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8UqDC4MfKSe",
        "outputId": "47d25777-02d6-4f98-897c-bee1343237c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}