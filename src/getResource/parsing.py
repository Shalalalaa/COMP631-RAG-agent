import requests
from bs4 import BeautifulSoup

# ç›®æ ‡ç½‘ç«™
BASE_URL = "https://www.zgjmorg.com/"

def get_category_links():
    """çˆ¬å–å‘¨å…¬è§£æ¢¦åˆ†ç±»é¡µé¢ï¼Œè·å–æ‰€æœ‰æ¢¦å¢ƒåˆ†ç±»çš„é“¾æ¥"""
    response = requests.get(BASE_URL, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, "html.parser")

    category_links = {}

    # é€‰å–åˆ†ç±»éƒ¨åˆ†
    category_div = soup.select_one("div.left")  # âœ… é€‰æ‹©æ­£ç¡®çš„ div
    for link in category_div.select("a"):  # âœ… é€‰æ‹©æ‰€æœ‰ <a> æ ‡ç­¾
        category_name = link.text.strip()
        category_url = BASE_URL + link["href"]  # æ‹¼æ¥å®Œæ•´ URL

        category_links[category_name] = category_url

    return category_links

# æµ‹è¯•æ˜¯å¦èƒ½æ­£ç¡®è·å–åˆ†ç±»
category_links = get_category_links()
print(category_links)  # âœ… è¿è¡Œååº”è¾“å‡º {'äº¤é€šè¿è¾“': 'https://www.example.com/wupin/jiaotong/', ...}


# import os
# import json
# import requests
# from bs4 import BeautifulSoup
# import time

# # ç›®æ ‡ç½‘ç«™
# BASE_URL = "https://www.zgjmorg.com/"  # æ›¿æ¢æˆçœŸå®çš„å‘¨å…¬è§£æ¢¦ç½‘ç«™

# # åˆ›å»ºå­˜å‚¨ç›®å½•
# os.makedirs("data/ZhouGong", exist_ok=True)

# def get_category_links():
#     """çˆ¬å–å‘¨å…¬è§£æ¢¦åˆ†ç±»é¡µé¢ï¼Œè·å–æ‰€æœ‰æ¢¦å¢ƒåˆ†ç±»çš„é“¾æ¥"""
#     response = requests.get(BASE_URL, headers={"User-Agent": "Mozilla/5.0"})
#     soup = BeautifulSoup(response.text, "html.parser")

#     category_links = {}
#     for link in soup.select("a.right"):  # âœ… ä¿®æ”¹ä¸ºæ­£ç¡®çš„ class
#         category_name = link.text.strip()
#         category_url = link["href"]
#         category_links[category_name] = category_url

#     return category_links

# def scrape_category(category_name, category_url):
#     """çˆ¬å–æŸä¸ªæ¢¦å¢ƒåˆ†ç±»çš„æ‰€æœ‰æ–‡ç« ï¼Œå¹¶å­˜ä¸ºå•ç‹¬çš„ JSON æ–‡ä»¶"""
#     response = requests.get(category_url, headers={"User-Agent": "Mozilla/5.0"})
#     soup = BeautifulSoup(response.text, "html.parser")

#     dreams_data = []
#     articles = soup.select("div.dream-item")  # âœ… ä¿®æ”¹ä¸ºæ­£ç¡®çš„ class
#     for article in articles:
#         title = article.select_one("h2.dream-title").text.strip()  # âœ… æ ‡é¢˜
#         content = article.select_one("p.dream-content").text.strip()  # âœ… è§£æå†…å®¹

#         # å­˜å…¥å½“å‰åˆ†ç±»çš„åˆ—è¡¨
#         dreams_data.append({"title": title, "content": content})

#         time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°

#     # å­˜ JSON
#     json_filename = f"data/ZhouGong/{category_name}.json"
#     with open(json_filename, "w", encoding="utf-8") as f:
#         json.dump(dreams_data, f, ensure_ascii=False, indent=4)

#     print(f"âœ… å·²ä¿å­˜ï¼š{json_filename}")

# # è·å–æ‰€æœ‰åˆ†ç±»é“¾æ¥
# category_links = get_category_links()

# # éå†æ¯ä¸ªåˆ†ç±»ï¼Œçˆ¬å–æ•°æ®å¹¶å­˜ JSON
# for category_name, category_url in category_links.items():
#     scrape_category(category_name, category_url)

# print("ğŸ‰ æ‰€æœ‰åˆ†ç±»çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜ï¼")
