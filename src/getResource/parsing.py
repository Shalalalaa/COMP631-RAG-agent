import os
import json
import requests
from bs4 import BeautifulSoup
import time

BASE_URL = "https://www.zgjmorg.com"

# create data directoyr
os.makedirs("data/ZhouGong", exist_ok=True)

def get_all_pages(category_url):
    response = requests.get(category_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, "html.parser")

    page_urls = [category_url]
    for link in soup.select("div.list-pages a"):
        href = link["href"]
        if "list" in href and href not in page_urls:
            full_url = BASE_URL + href if href.startswith("/") else href
            page_urls.append(full_url)

    return page_urls

# get dreams under one category
def get_dream_links(category_url):
    response = requests.get(category_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, "html.parser")

    dream_links = []
    for link in soup.select("div#list ul li a"):
        title = link.text.strip()
        href = link["href"]
        full_url = BASE_URL + href if href.startswith("/") else href
        dream_links.append({"title": title, "url": full_url})

    return dream_links

# get dream content
def scrape_dream_content(dream_url):
    response = requests.get(dream_url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(response.text, "html.parser")

    # get title
    title = soup.select_one("div#entrytitle h1").text.strip() if soup.select_one("div#entrytitle h1") else "æ— æ ‡é¢˜"

    # get content
    content = []
    content_blocks = soup.select("div.read-content p")
    for block in content_blocks:
        text = block.text.strip()
        if text:
            content.append(text)

    return {"title": title, "content": "\n".join(content)}

# store it to json file
def scrape_category(category_name, category_url):
    all_pages = get_all_pages(category_url)
    all_dreams = []

    for page_url in all_pages:
        print(f"â³ æ­£åœ¨çˆ¬å–åˆ†ç±»: {category_name} | é¡µé¢: {page_url}")
        dream_links = get_dream_links(page_url)

        for i, dream in enumerate(dream_links):
            # test 10 first
            if i >= 10:  
                break

            print(f"ğŸ” çˆ¬å–æ¢¦å¢ƒ: {dream['title']} - {dream['url']}")
            dream_data = scrape_dream_content(dream["url"])
            all_dreams.append(dream_data)

            time.sleep(2)

    json_filename = f"data/ZhouGong/{category_name}.json"
    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump(all_dreams, f, ensure_ascii=False, indent=4)

    print(f"Complete: {json_filename}")

# test
test_category = "äººç‰©ç±»"
test_url = "https://www.zgjmorg.com/renwu/"
scrape_category(test_category, test_url)












# import os
# import json
# import requests
# from bs4 import BeautifulSoup
# import time

# # ç›®æ ‡ç½‘ç«™
# BASE_URL = "https://www.zgjmorg.com/"  # æ›¿æ¢æˆçœŸå®çš„å‘¨å…¬è§£æ¢¦ç½‘ç«™

# # åˆ›å»ºå­˜å‚¨ç›®å½•
# os.makedirs("data/ZhouGong", exist_ok=True)

# def get_category_links():
#     """çˆ¬å–å‘¨å…¬è§£æ¢¦åˆ†ç±»é¡µé¢ï¼Œè·å–æ‰€æœ‰æ¢¦å¢ƒåˆ†ç±»çš„é“¾æ¥"""
#     response = requests.get(BASE_URL, headers={"User-Agent": "Mozilla/5.0"})
#     soup = BeautifulSoup(response.text, "html.parser")

#     category_links = {}
#     for link in soup.select("a.right"):  # âœ… ä¿®æ”¹ä¸ºæ­£ç¡®çš„ class
#         category_name = link.text.strip()
#         category_url = link["href"]
#         category_links[category_name] = category_url

#     return category_links

# def scrape_category(category_name, category_url):
#     """çˆ¬å–æŸä¸ªæ¢¦å¢ƒåˆ†ç±»çš„æ‰€æœ‰æ–‡ç« ï¼Œå¹¶å­˜ä¸ºå•ç‹¬çš„ JSON æ–‡ä»¶"""
#     response = requests.get(category_url, headers={"User-Agent": "Mozilla/5.0"})
#     soup = BeautifulSoup(response.text, "html.parser")

#     dreams_data = []
#     articles = soup.select("div.dream-item")  # âœ… ä¿®æ”¹ä¸ºæ­£ç¡®çš„ class
#     for article in articles:
#         title = article.select_one("h2.dream-title").text.strip()  # âœ… æ ‡é¢˜
#         content = article.select_one("p.dream-content").text.strip()  # âœ… è§£æå†…å®¹

#         # å­˜å…¥å½“å‰åˆ†ç±»çš„åˆ—è¡¨
#         dreams_data.append({"title": title, "content": content})

#         time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°

#     # å­˜ JSON
#     json_filename = f"data/ZhouGong/{category_name}.json"
#     with open(json_filename, "w", encoding="utf-8") as f:
#         json.dump(dreams_data, f, ensure_ascii=False, indent=4)

#     print(f"âœ… å·²ä¿å­˜ï¼š{json_filename}")

# # è·å–æ‰€æœ‰åˆ†ç±»é“¾æ¥
# category_links = get_category_links()

# # éå†æ¯ä¸ªåˆ†ç±»ï¼Œçˆ¬å–æ•°æ®å¹¶å­˜ JSON
# for category_name, category_url in category_links.items():
#     scrape_category(category_name, category_url)

# print("ğŸ‰ æ‰€æœ‰åˆ†ç±»çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜ï¼")
